[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "from cjm_pytorch_utils.core import get_torch_device\ndevice = get_torch_device()\ndtype = torch.float16 if device == 'cuda' else torch.float32\ndevice, dtype\n\n('cpu', torch.float32)\n\n\n\nsource\n\npil_to_latent\n\n pil_to_latent (image:<module'PIL.Image'from'/opt/hostedtoolcache/Python/3\n                .9.16/x64/lib/python3.9/site-packages/PIL/Image.py'>,\n                vae:diffusers.models.autoencoder_kl.AutoencoderKL)\n\nThis function converts an image to latents using a VAE model.\nReturns: latents (torch.Tensor): The latents generated from the image.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimage\nImage\nThe image to be converted to latents.\n\n\nvae\nAutoencoderKL\nThe VAE model used to convert the image to latents.\n\n\n\nSelect a model\n\nmodel_name = \"stabilityai/stable-diffusion-2-1\"\n\nLoad autoencoder\n\nvae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\").to(device=device, dtype=dtype)\n\nOpen sample image\n\nimg_path = '../images/cat.jpg'\nsrc_img = Image.open(img_path).convert('RGB')\nsrc_img\n\n\n\n\nEncode image\n\nimg_latents = pil_to_latent(src_img, vae)\nimg_latents.shape\n\ntorch.Size([1, 4, 64, 96])\n\n\n\nsource\n\n\nlatent_to_pil\n\n latent_to_pil (latents:torch.Tensor,\n                vae:diffusers.models.autoencoder_kl.AutoencoderKL)\n\nThis function converts latents to an image using a VAE model.\nReturns: image (PIL.Image): The image generated from the latents.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nlatents\ntorch.Tensor\nThe latents to be converted to an image.\n\n\nvae\nAutoencoderKL\nThe VAE model used to convert the latents to an image.\n\n\n\nDecode latents\n\ndecoded_img = latent_to_pil(img_latents, vae)\ndecoded_img\n\n\n\n\n\nsource\n\n\ntext_to_emb\n\n text_to_emb (prompt:str,\n              tokenizer:transformers.models.clip.tokenization_clip.CLIPTok\n              enizer, text_encoder:transformers.models.clip.modeling_clip.\n              CLIPTextModel, negative_prompt:str='', maxlen:int=None)\n\nEncodes the provided text prompts using the specified text encoder.\nReturns: torch.Tensor: The encoded text.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprompt\nstr\n\nThe text prompt to be encoded.\n\n\ntokenizer\nCLIPTokenizer\n\nThe tokenizer to be used.\n\n\ntext_encoder\nCLIPTextModel\n\nThe text encoder to be used.\n\n\nnegative_prompt\nstr\n\nThe negative text prompt to be encoded.\n\n\nmaxlen\nint\nNone\nThe maximum length of the encoded text. Default is None.\n\n\n\nLoad tokenizer\n\n# Load the tokenizer for the specified model\ntokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n\nLoad text encoder\n\n# Load the text encoder for the specified model\ntext_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\").to(device=device, dtype=dtype)\n\nDefine sample prompt\n\nprompt = \"A cat sitting on the floor.\"\n\nEncode sample prompt\n\ntext_emb = text_to_emb(prompt, tokenizer, text_encoder)\ntext_emb.shape\n\ntorch.Size([2, 77, 1024])\n\n\n\nsource\n\n\nprepare_noise_scheduler\n\n prepare_noise_scheduler (noise_scheduler, max_steps:int=50,\n                          noise_strength:float=1.0)\n\nPrepare the noise scheduler by setting the timesteps and adjusting the noise strength.\nReturns: noise_scheduler (object): The modified noise scheduler object\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnoise_scheduler\n\n\nThe noise scheduler object to be modified\n\n\nmax_steps\nint\n50\nThe maximum number of steps\n\n\nnoise_strength\nfloat\n1.0\nThe strength of the noise\n\n\n\nLoad noise scheduler\n\nnoise_scheduler = DEISMultistepScheduler.from_pretrained(model_name, subfolder='scheduler')\n\nprint(f\"Number of timesteps: {len(noise_scheduler.timesteps)}\")\nnoise_scheduler.timesteps[:10]\n\nNumber of timesteps: 1000\n\n\ntensor([999., 998., 997., 996., 995., 994., 993., 992., 991., 990.])\n\n\nUpdate noise scheduler\n\nnoise_scheduler = prepare_noise_scheduler(noise_scheduler, 25, 1.0)\n\nprint(f\"Number of timesteps: {len(noise_scheduler.timesteps)}\")\nnoise_scheduler.timesteps[:10]\n\nNumber of timesteps: 25\n\n\ntensor([999, 959, 919, 879, 839, 799, 759, 719, 679, 639])\n\n\n\nsource\n\n\nprepare_depth_mask\n\n prepare_depth_mask (depth_map, divisor=8)\n\nPrepare the depth mask by resizing and normalizing the depth map.\nReturns: depth_mask (torch.Tensor): The normalized and resized depth mask\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndepth_map\n\n\nThe depth map image\n\n\ndivisor\nint\n8\nThe divisor value used to resize the depth map\n\n\n\nLoad depth map\n\ndepth_map_path = '../images/depth-cat.png'\ndepth_map = Image.open(depth_map_path)\ndepth_map\n\n\n\n\nPrepare depth mask\n\ndepth_mask = prepare_depth_mask(depth_map).to(device=device, dtype=dtype)\ndepth_mask.shape, depth_mask.min(), depth_mask.max()\n\n(torch.Size([1, 1, 64, 96]),\n tensor(-1., device='cuda:0', dtype=torch.float16),\n tensor(1., device='cuda:0', dtype=torch.float16))\n\n\n\nsource\n\n\ndenoise_depth2img\n\n denoise_depth2img (latents:torch.Tensor, depth_mask:torch.Tensor,\n                    text_emb:torch.Tensor, unet:diffusers.models.unet_2d_c\n                    ondition.UNet2DConditionModel, noise_scheduler,\n                    guidance_scale:float=8.0)\n\nGenerate an image from a given initial image, depth map and prompt.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatents\nTensor\n\nThe initial image latents\n\n\ndepth_mask\nTensor\n\nThe image depth mask\n\n\ntext_emb\nTensor\n\nThe embedded text prompt and negative prompt\n\n\nunet\nUNet2DConditionModel\n\nThe Unet denoiser\n\n\nnoise_scheduler\n\n\nThe noise scheduler\n\n\nguidance_scale\nfloat\n8.0\nThe guidance scale\n\n\n\n\nmodel_name = \"stabilityai/stable-diffusion-2-depth\"\nunet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\").to(device=device, dtype=dtype)\nvae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\").to(device=device, dtype=dtype)\n\ntokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\ntext_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\").to(device=device, dtype=dtype)\n\nnoise_scheduler = DEISMultistepScheduler.from_pretrained(model_name, subfolder='scheduler')\n\n\nnoise_scheduler = prepare_noise_scheduler(noise_scheduler, 25, 0.9)\nimg_latents = pil_to_latent(src_img, vae).to(device=device, dtype=dtype)\n\nprompt = \"A Monet oil painting of a cat\"\nnegative_prompt = \"bad, deformed, ugly, bad anotomy\"\ntext_emb = text_to_emb(prompt=prompt, \n                       tokenizer=tokenizer, \n                       text_encoder=text_encoder, \n                       negative_prompt=negative_prompt)\n\n# Generate latent noise\nnoise = torch.randn(img_latents.shape, device=unet.device, dtype=unet.dtype)\n\n# Add noise to the image latents at the first timestep\nlatents = noise_scheduler.add_noise(img_latents, \n                                    noise, \n                                    noise_scheduler.timesteps[[0]]).to(unet.device)\n\ndenoised_latents = denoise_depth2img(latents=latents,\n                                     depth_mask=depth_mask,\n                                     text_emb=text_emb,\n                                     unet=unet,\n                                     noise_scheduler=noise_scheduler,\n                                     guidance_scale=8.0)\n\nlatent_to_pil(denoised_latents, vae)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cjm-diffusers-utils",
    "section": "",
    "text": "pip install cjm_diffusers_utils"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "cjm-diffusers-utils",
    "section": "How to use",
    "text": "How to use\n\nimport torch\nfrom cjm_pytorch_utils.core import get_torch_device\ndevice = get_torch_device()\ndtype = torch.float16 if device == 'cuda' else torch.float32\ndevice, dtype\n\n('cuda', torch.float16)\n\n\n\npil_to_latent\n\nfrom cjm_diffusers_utils.core import pil_to_latent\nfrom PIL import Image\nfrom diffusers import AutoencoderKL\n\n\nmodel_name = \"stabilityai/stable-diffusion-2-1\"\nvae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\").to(device=device, dtype=dtype)\n\n\nimg_path = img_path = '../images/cat.jpg'\nsrc_img = Image.open(img_path).convert('RGB')\nprint(f\"Source Image Size: {src_img.size}\")\n\nimg_latents = pil_to_latent(src_img, vae)\nprint(f\"Latent Dimensions: {img_latents.shape}\")\n\nSource Image Size: (768, 512)\nLatent Dimensions: torch.Size([1, 4, 64, 96])\n\n\n\n\nlatent_to_pil\n\nfrom cjm_diffusers_utils.core import latent_to_pil\n\n\ndecoded_img = latent_to_pil(img_latents, vae)\nprint(f\"Decoded Image Size: {decoded_img.size}\")\n\nDecoded Image Size: (768, 512)\n\n\n\n\ntext_to_emb\n\nfrom cjm_diffusers_utils.core import text_to_emb\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\n# Load the tokenizer for the specified model\ntokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n# Load the text encoder for the specified model\ntext_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\").to(device=device, dtype=dtype)\n\n\nprompt = \"A cat sitting on the floor.\"\ntext_emb = text_to_emb(prompt, tokenizer, text_encoder)\ntext_emb.shape\n\ntorch.Size([2, 77, 1024])\n\n\n\n\nprepare_noise_scheduler\n\nfrom cjm_diffusers_utils.core import prepare_noise_scheduler\nfrom diffusers import DEISMultistepScheduler\n\n\nnoise_scheduler = DEISMultistepScheduler.from_pretrained(model_name, subfolder='scheduler')\nprint(f\"Number of timesteps: {len(noise_scheduler.timesteps)}\")\nprint(noise_scheduler.timesteps[:10])\n\nnoise_scheduler = prepare_noise_scheduler(noise_scheduler, 70, 1.0)\nprint(f\"Number of timesteps: {len(noise_scheduler.timesteps)}\")\nprint(noise_scheduler.timesteps[:10])\n\nNumber of timesteps: 1000\ntensor([999., 998., 997., 996., 995., 994., 993., 992., 991., 990.])\nNumber of timesteps: 70\ntensor([999, 985, 970, 956, 942, 928, 913, 899, 885, 871])\n\n\n\n\nprepare_depth_mask\n\nfrom cjm_diffusers_utils.core import prepare_depth_mask\n\n\ndepth_map_path = '../images/depth-cat.png'\ndepth_map = Image.open(depth_map_path)\nprint(f\"Depth map size: {depth_map.size}\")\n\ndepth_mask = prepare_depth_mask(depth_map).to(device=device, dtype=dtype)\ndepth_mask.shape, depth_mask.min(), depth_mask.max()\n\nDepth map size: (768, 512)\n\n\n(torch.Size([1, 1, 64, 96]),\n tensor(-1., device='cuda:0', dtype=torch.float16),\n tensor(1., device='cuda:0', dtype=torch.float16))"
  }
]